{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d5e4930-621a-4b6d-ba2c-eb608bc9b7ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(337344, 11)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 337344 entries, 0 to 337343\n",
      "Data columns (total 11 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   MEMBER_ID     337344 non-null  object\n",
      " 1   CATALOG_NM    337344 non-null  object\n",
      " 2   CATEGORYM_ID  337344 non-null  int64 \n",
      " 3   KEYWORD       337344 non-null  object\n",
      " 4   DISPLAY       337344 non-null  int64 \n",
      " 5   CATALOG_DESC  337344 non-null  object\n",
      " 6   INPUT_DT      337344 non-null  int64 \n",
      " 7   UPDATE_DT     337344 non-null  int64 \n",
      " 8   REMOTEIP      337344 non-null  int64 \n",
      " 9   SHOW_CHK      337344 non-null  int64 \n",
      " 10  judge         337344 non-null  int64 \n",
      "dtypes: int64(7), object(4)\n",
      "memory usage: 28.3+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "products_concat = pd.read_csv('./dataset/product_concat_v1.csv') \n",
    "print(products_concat.shape)\n",
    "print(products_concat.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7376b7-ecd3-4606-9b54-d28ffd9bfd5f",
   "metadata": {},
   "source": [
    "# 소문자, 불용어, 어근 추출, 단어 토큰 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5cd5c8b-4324-4596-99c2-97fb25470a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "# 문장에 포함된 구두점을 삭제하기 위한 맵핑 정보 생성\n",
    "# ord() : 매개변수로 전달된 문자에 해당하는 유니코드를 반환\n",
    "# 구두점이 발견되면 : None값으로 대채 -> 즉, 지운다는 뜻\n",
    "\n",
    "remove_punct_dict = {ord(punct):None for punct in string.punctuation }\n",
    "\n",
    "lemmar = WordNetLemmatizer()\n",
    "\n",
    "# 문장 입력받음 -> stopwords를 제거 -> 소문자로 변환 -> 단어로 토큰->어근 변환\n",
    "def LenNormalize(text):\n",
    "    tokens = nltk.word_tokenize(text.lower().translate(remove_punct_dict))\n",
    "    return [lemmar.lemmatize(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7aa10d27-66d7-4dd7-971a-55b31a0a3b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cnt_vect = CountVectorizer(tokenizer=LenNormalize, stop_words='english', ngram_range = (1,2))\n",
    "\n",
    "catalog_nm_vect = cnt_vect.fit_transform(products_concat[['CATALOG_NM']])\n",
    "keyword_vect = cnt_vect.fit_transform(products_concat[['KEYWORD']])\n",
    "catalog_desc_vect = cnt_vect.fit_transform(products_concat[['CATALOG_DESC']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0fb9d5c8-9958-4793-8907-5e24928a7026",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'csr_matrix' object has no attribute 'vocabulary_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCATALOG_NM\u001b[39m\u001b[38;5;124m'\u001b[39m, catalog_nm_vect\u001b[38;5;241m.\u001b[39mvocabulary_)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKEYWORD\u001b[39m\u001b[38;5;124m'\u001b[39m, keyword_vect)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCATALOG_DESC\u001b[39m\u001b[38;5;124m'\u001b[39m, catalog_desc_vect)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'csr_matrix' object has no attribute 'vocabulary_'"
     ]
    }
   ],
   "source": [
    "print('CATALOG_NM', catalog_nm_vect.vocabulary_)\n",
    "print('KEYWORD', keyword_vect)\n",
    "print('CATALOG_DESC', catalog_desc_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0736a121-88fa-44ce-b9b0-26d4362940c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'shangyuanshibang'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m x \u001b[38;5;241m=\u001b[39m products_concat\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjudge\u001b[39m\u001b[38;5;124m'\u001b[39m, axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m corr_matrix \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mcorr()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:10054\u001b[0m, in \u001b[0;36mDataFrame.corr\u001b[1;34m(self, method, min_periods, numeric_only)\u001b[0m\n\u001b[0;32m  10052\u001b[0m cols \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m  10053\u001b[0m idx \u001b[38;5;241m=\u001b[39m cols\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m> 10054\u001b[0m mat \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto_numpy(dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m, na_value\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mnan, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m  10056\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpearson\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m  10057\u001b[0m     correl \u001b[38;5;241m=\u001b[39m libalgos\u001b[38;5;241m.\u001b[39mnancorr(mat, minp\u001b[38;5;241m=\u001b[39mmin_periods)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:1838\u001b[0m, in \u001b[0;36mDataFrame.to_numpy\u001b[1;34m(self, dtype, copy, na_value)\u001b[0m\n\u001b[0;32m   1836\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1837\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(dtype)\n\u001b[1;32m-> 1838\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mas_array(dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy, na_value\u001b[38;5;241m=\u001b[39mna_value)\n\u001b[0;32m   1839\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dtype:\n\u001b[0;32m   1840\u001b[0m     result \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(result, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:1732\u001b[0m, in \u001b[0;36mBlockManager.as_array\u001b[1;34m(self, dtype, copy, na_value)\u001b[0m\n\u001b[0;32m   1730\u001b[0m         arr\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwriteable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1731\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1732\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interleave(dtype\u001b[38;5;241m=\u001b[39mdtype, na_value\u001b[38;5;241m=\u001b[39mna_value)\n\u001b[0;32m   1733\u001b[0m     \u001b[38;5;66;03m# The underlying data was copied within _interleave, so no need\u001b[39;00m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;66;03m# to further copy if copy=True or setting na_value\u001b[39;00m\n\u001b[0;32m   1736\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:1794\u001b[0m, in \u001b[0;36mBlockManager._interleave\u001b[1;34m(self, dtype, na_value)\u001b[0m\n\u001b[0;32m   1792\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1793\u001b[0m         arr \u001b[38;5;241m=\u001b[39m blk\u001b[38;5;241m.\u001b[39mget_values(dtype)\n\u001b[1;32m-> 1794\u001b[0m     result[rl\u001b[38;5;241m.\u001b[39mindexer] \u001b[38;5;241m=\u001b[39m arr\n\u001b[0;32m   1795\u001b[0m     itemmask[rl\u001b[38;5;241m.\u001b[39mindexer] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1797\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m itemmask\u001b[38;5;241m.\u001b[39mall():\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'shangyuanshibang'"
     ]
    }
   ],
   "source": [
    "x = products_concat.drop('judge', axis = 1)\n",
    "corr_matrix = x.corr()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
