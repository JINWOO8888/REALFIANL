


import pandas as pd

products = pd.read_csv('./dataset/product_concat_v2.csv')
print(products.shape)
print(products.info())


from nltk.stem import WordNetLemmatizer
import nltk
import string

# 문장에 포함된 구두점을 삭제하기 위한 맵핑 정보 생성
# ord() : 매개변수로 전달된 문자에 해당하는 유니코드를 반환
# 구두점이 발견되면 : None값으로 대채 -> 즉, 지운다는 뜻

remove_punct_dict = {ord(punct):None for punct in string.punctuation }

lemmar = WordNetLemmatizer()

# 문장 입력받음 -> stopwords를 제거 -> 소문자로 변환 -> 단어로 토큰->어근 변환
def LenNormalize(text):
    tokens = nltk.word_tokenize(text.lower().translate(remove_punct_dict))
    return [lemmar.lemmatize(token) for token in tokens]


from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
import warnings
warnings.filterwarnings('ignore')

tfidf_vect = TfidfVectorizer(tokenizer=LenNormalize, stop_words='english', ngram_range = (1,1))
catalog_desc_vect = tfidf_vect.fit_transform(products['CATALOG_DESC'])


from sklearn.naive_bayes import MultinomialNB, CategoricalNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix

# x = tfidf_vect.vocabulary_
x = catalog_desc_vect
y = products['judge']

x_train, x_test, y_train, y_test = train_test_split(x,y,test_size= 0.3,random_state= 0, stratify= y)


from sklearn.preprocessing import normalize
from scipy.sparse import hstack

normalized_vect = normalize(x_train)


x_train = x_train.astype('float32')


x_train.toarray()[0]


model = CategoricalNB()
model.fit(x_train.toarray(), y_train)


y_hat = model.predict(x_test)
con_mat = confusion_matrix(y_test,y_hat)
print(con_mat)
