import pandas as pd
import warnings
warnings.filterwarnings('ignore')
import seaborn as sns
import matplotlib.pyplot as plt

import pandas as pd
products_concat = pd.read_csv('./dataset/product_concat_v1.csv') 
print(products_concat.shape)
print(products_concat.info())





from nltk.stem import WordNetLemmatizer
import nltk
import string

# 문장에 포함된 구두점을 삭제하기 위한 맵핑 정보 생성
# ord() : 매개변수로 전달된 문자에 해당하는 유니코드를 반환
# 구두점이 발견되면 : None값으로 대채 -> 즉, 지운다는 뜻

remove_punct_dict = {ord(punct):None for punct in string.punctuation }

lemmar = WordNetLemmatizer()

# 문장 입력받음 -> stopwords를 제거 -> 소문자로 변환 -> 단어로 토큰->어근 변환
def LenNormalize(text):
    tokens = nltk.word_tokenize(text.lower().translate(remove_punct_dict))
    return [lemmar.lemmatize(token) for token in tokens]


from sklearn.feature_extraction.text import CountVectorizer

cnt_vect = CountVectorizer(tokenizer=LenNormalize, stop_words='english', ngram_range = (1,2))

catalog_nm_vect = cnt_vect.fit_transform(products_concat['CATALOG_NM'])
keyword_vect = cnt_vect.fit_transform(products_concat['KEYWORD'])
catalog_desc_vect = cnt_vect.fit_transform(products_concat['CATALOG_DESC'])





# print('CATALOG_NM', catalog_nm_vect)
# print('KEYWORD', keyword_vect)
# print('CATALOG_DESC', catalog_desc_vect)

catalog_nm_df = pd.DataFrame(catalog_nm_vect.toarray(), columns=catalog_nm_vect.get_feature_names_out())
keyword_df = pd.DataFrame(keyword_vect.toarray(), columns=keyword_vect.get_feature_names_out())
catalog_desc_df = pd.DataFrame(catalog_desc_vect.toarray(), columns=catalog_desc_vect.get_feature_names_out())


# print(catalog_nm_df[)
# print(keyword_df[0])
# print(catalog_desc_df[0])





sns.pairplot(products_concat)
plt.show()


# 





x = products_concat.drop('judge', axis = 1)
corr_matrix = x.corr()
