


import pandas as pd

#Weird_members = pd.read_csv('./dataset/이상회원데이터v2.csv')
#Normal_members = pd.read_csv('./dataset/정상회원데이터v2.csv')
Weird_products = pd.read_csv('./dataset/이상상품_V2.csv')
Normal_products = pd.read_csv('./dataset/정상상품_V2.csv')
#BlackLists = pd.read_csv('./dataset/블랙리스트_v2.csv')
#BlockKeywords = pd.read_csv('./dataset/금지어keyword.csv')






print('이상회원:', Weird_members.shape)
print('정상회원:', Normal_members.shape)
print('이상상품:', Weird_products.shape)
print('정상상품:', Normal_products.shape)
print('블랙리스트:', BlackLists.shape)


print('이상회원')

print(Weird_members.info())


print('정상회원')
print(Normal_members.info())


print('이상물품')
print(Weird_products.info())


print('정상회원')
print(Normal_products.info())





Normal_products['judge'] = 1
Weird_products['judge'] = 0

print(Weird_products.shape)
print(Normal_products.shape)


concat_products = pd.concat([Normal_products,Weird_products],ignore_index=True)

print(concat_products.shape)
print(concat_products.info())


print(concat_products.isna().sum())
# concat_item.dropna(subset=['KEYWORD'],inplace=True)
#print(Concat_products.shape)


concat_products[concat_products['CATALOG_DESC'].isna()]


print(concat_products.dtypes)

concat_products['CATEGORYM_ID'] = concat_products['CATEGORYM_ID'].astype(str)
concat_products['INPUT_DT'] = concat_products['INPUT_DT'].astype(str)
concat_products['UPDATE_DT'] = concat_products['UPDATE_DT'].astype(str)
print(concat_products.dtypes)








for keyword in concat_products['KEYWORD'].str.split(', '):
    print(keyword)

def split_comma(df):
    for word in df.str.split(', '):
        return keyword

concat_products['KEYWORD'].apply(lambda x: split_comma(x))








import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, WordNetLemmatizer
import string


def preprocess_text(text):
    if isinstance(text, str):  # Check if the value is a string
        # Convert to lowercase
        text = text.lower()

        # Remove special characters
        text = ''.join([char for char in text if char not in string.punctuation])

    return text

def remove_stopwords(text):
    if isinstance(text, str):  # Check if the value is a string
        stop_words = set(stopwords.words('english'))
        words = text.split()
        filtered_words = [word for word in words if word.lower() not in stop_words]
        return ' '.join(filtered_words)
    return text

def apply_stemming(text):
    if isinstance(text, str):  # Check if the value is a string
        stemmer = PorterStemmer()
        words = text.split()
        stemmed_words = [stemmer.stem(word) for word in words]
        return ' '.join(stemmed_words)
    return text


from nltk.tokenize import word_tokenize
concat_products['CATALOG_NM'] = concat_products['CATALOG_NM'].apply(lambda x:word_tokenize(x))
concat_products['CATALOG_NM'] = concat_products['CATALOG_NM'].apply(lambda x:preprocess_text(x))
concat_products['CATALOG_NM'] = concat_products['CATALOG_NM'].apply(lambda x:apply_stemming(x))
concat_products['CATALOG_NM'] = concat_products['CATALOG_NM'].apply(lambda x:remove_stopwords(x))


display(concat_products['CATALOG_NM'])


from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer


cv = CountVectorizer()






from sklearn.cluster import KMeans
from nltk.stem import WordNetLemmatizer
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import numpy as np


remove_punct_dict = { ord(punct):None for punct in string.punctuation } 
lemmar = WordNetLemmatizer()

def LemNormalize(text):
    tokens = nltk.word_tokenize(text.lower().translate(remove_punct_dict)) # 소문자 변환 -> 구두점 제거
    return [lemmar.lemmatize(token) for token in tokens ] # 어근 찾기
    

