{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1000827859.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[5], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    python setup.py install\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "python setup.py install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpunkt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stopwords\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Word2Vec\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n\u001b[0;32m     11\u001b[0m products \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./dataset/products_no_china_no_token_v2.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "products = pd.read_csv('./dataset/products_no_china_no_token_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 데이터를 단어 단위로 토큰화\n",
    "def string_to_list(input_string):\n",
    "# 문자열을 rhdqor()를 기준으로 나누어 리스트로 변환\n",
    "  return str(input_string).split(' ')\n",
    "\n",
    "\n",
    "# string_to_list 함수를 사용하여 각 문장을 단어의 리스트로 변환\n",
    "sentences = [string_to_list(sentence) for sentence in products['CATALOG_DESC']]\n",
    "\n",
    "# Word2Vec 모델 초기화\n",
    "model = Word2Vec(sentences=sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "# 이 때, vector_size는 단어 벡터의 차원 수를 지정합니다.\n",
    "# window는 주변 단어의 범위를 나타냅니다.\n",
    "# min_count는 최소 등장 횟수를 의미합니다.\n",
    "\n",
    "# 모델 학습\n",
    "model.train(sentences, total_examples=len(sentences), epochs=10)\n",
    "\n",
    "# 단어 벡터 확인\n",
    "#word_vector = model.wv['document']\n",
    "#print(word_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "keywords = pd.read_csv('/content/drive/MyDrive/REALFIANL/dataset/IPR리스트_231218.csv')\n",
    "\n",
    "keywords['KEYWORD'] = keywords['KEYWORD'].apply(lambda x: re.sub('[^0-9a-zA-Z]', ' ', x))\n",
    "\n",
    "keywords['KEYWORD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_list = []\n",
    "\n",
    "for word in keywords['KEYWORD']:\n",
    "  keyword_list.append(word)\n",
    "\n",
    "keyword_list[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# 두 단어의 코사인 유사도 계산 함수\n",
    "def cosine_similarity(word1, word2):\n",
    "    vec1 = model.wv[word1].reshape(1, -1)  # (1, 100)의 형태로 reshape\n",
    "    vec2 = model.wv[word2]\n",
    "    similarity = np.dot(vec1, vec2.T) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "    return similarity[0, 0]  # 결과를 스칼라 값으로 얻기 위해 [0, 0]을 사용\n",
    "\n",
    "# 예시: 'apetamin'과 다른 단어들 간의 유사도 계산\n",
    "other_words_vectors = [model.wv[word] for word in sentences]\n",
    "\n",
    "\n",
    "# 인덱스와 유사도를 데이터프레임에 추가\n",
    "similarity = cosine_similarity(keyword_list, other_words_vectors)    \n",
    "# products['SIMILARITY'] = similarity[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 두 단어의 코사인 유사도 계산 함수\n",
    "def cosine_similarity(word1, word2):\n",
    "    vec1 = model.wv[word1].reshape(1, -1)  # (1, 100)의 형태로 reshape\n",
    "    vec2 = model.wv[word2].reshape(1, -1)  # (1, 100)의 형태로 reshape\n",
    "    similarity = np.dot(vec1, vec2.T) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "    return similarity[0, 0]  # 결과를 스칼라 값으로 얻기 위해 [0, 0]을 사용\n",
    "\n",
    "# 예시: 'apetamin'과 다른 단어들 간의 유사도 계산\n",
    "other_words_vectors = [model.wv[word] for word in sentences]\n",
    "\n",
    "# 유사도 계산\n",
    "similarities = [cosine_similarity(keyword_list, word) for word in other_words_vectors]\n",
    "\n",
    "# 데이터프레임에 추가\n",
    "products['SIMILARITY'] = similarities"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
