{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "products = pd.read_csv('./dataset/product_concat_v3.csv')\n",
    "members = pd.read_csv('./dataset/members_concat_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(751315, 11)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 751315 entries, 0 to 751314\n",
      "Data columns (total 11 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   MEMBER_ID     751315 non-null  object\n",
      " 1   CATALOG_NM    751315 non-null  object\n",
      " 2   CATEGORYM_ID  751315 non-null  int64 \n",
      " 3   KEYWORD       751315 non-null  object\n",
      " 4   DISPLAY       751315 non-null  int64 \n",
      " 5   CATALOG_DESC  751259 non-null  object\n",
      " 6   INPUT_DT      751315 non-null  int64 \n",
      " 7   UPDATE_DT     751315 non-null  int64 \n",
      " 8   REMOTEIP      751315 non-null  int64 \n",
      " 9   SHOW_CHK      751315 non-null  int64 \n",
      " 10  judge         751315 non-null  int64 \n",
      "dtypes: int64(7), object(4)\n",
      "memory usage: 63.1+ MB\n"
     ]
    }
   ],
   "source": [
    "print(products.shape)\n",
    "products.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(466762, 14)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 466762 entries, 0 to 466761\n",
      "Data columns (total 14 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   MEMBER_ID   466762 non-null  object\n",
      " 1   GUBUN       466762 non-null  int64 \n",
      " 2   COUNTRY_CD  466579 non-null  object\n",
      " 3   KEYWORD     191511 non-null  object\n",
      " 4   LOGIN_DT    466762 non-null  int64 \n",
      " 5   LOGIN_CNT   466762 non-null  int64 \n",
      " 6   INPUT_DT    466762 non-null  int64 \n",
      " 7   UPDATE_DT   466762 non-null  int64 \n",
      " 8   REMOTEIP    466762 non-null  int64 \n",
      " 9   SHOW_CHK    466762 non-null  int64 \n",
      " 10  MTYPE       466762 non-null  int64 \n",
      " 11  ORDER_FLG   466762 non-null  int64 \n",
      " 12  judge       466762 non-null  int64 \n",
      " 13  BLACK_LIST  466762 non-null  int64 \n",
      "dtypes: int64(11), object(3)\n",
      "memory usage: 49.9+ MB\n"
     ]
    }
   ],
   "source": [
    "print(members.shape)\n",
    "members.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "# 문장에 포함된 구두점을 삭제하기 위한 맵핑 정보 생성\n",
    "# ord() : 매개변수로 전달된 문자에 해당하는 유니코드를 반환\n",
    "# 구두점이 발견되면 : None값으로 대채 -> 즉, 지운다는 뜻\n",
    "\n",
    "remove_punct_dict = {ord(punct):None for punct in string.punctuation }\n",
    "\n",
    "lemmar = WordNetLemmatizer()\n",
    "\n",
    "# 문장 입력받음 -> stopwords를 제거 -> 소문자로 변환 -> 단어로 토큰->어근 변환\n",
    "def LenNormalize(text):\n",
    "    tokens = nltk.word_tokenize(text.lower().translate(remove_punct_dict))\n",
    "    return [lemmar.lemmatize(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 상품설명에 결측치 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products['CATALOG_DESC'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "products.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(tokenizer= LenNormalize, stop_words= 'english', ngram_range = (1,1))\n",
    "desc_tfidf = tfidf_vect.fit_transform(products['CATALOG_DESC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(751259, 2254119)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desc_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "tfidf_vect_maxdf = TfidfVectorizer(tokenizer= LenNormalize, stop_words= 'english', ngram_range = (1,1), max_df= 0.5)\n",
    "desc_tfidf_maxdf = tfidf_vect_maxdf.fit_transform(products['CATALOG_DESC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(751259, 2254119)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desc_tfidf_maxdf.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
